<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Comparison of technologies for parallel computations on Intel Xeon Phi by MNie</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Comparison of technologies for parallel computations on Intel Xeon Phi</h1>
      <h2 class="project-tagline">XeonPhi, OpenCL, OpenMP, MPI, C++, Parallel computing</h2>
      <a href="https://github.com/MNie/ComparisonOfOpenCLOpenMPMPI" class="btn">View on GitHub</a>
      <a href="https://github.com/MNie/ComparisonOfOpenCLOpenMPMPI/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/MNie/ComparisonOfOpenCLOpenMPMPI/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="welcome" class="anchor" href="#welcome" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Welcome.</h3>

<p>This is a page of a engineer's degree repository. The main clue of this engineer's is to compare the most popular technologies for a parallel computations on Intel Xeon Phi Accelerator. Feel free to fork. Repository is under Apache License.</p>

<h3>
<a id="project-description" class="anchor" href="#project-description" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project description</h3>

<p>Following project resolves the issues, in the field of Computer Science, related to parallel computing. This is a type of calculation, wherein many instructions are executed simultaneously. Currently there are multiple existing technologies, which can be used in implementation of such a model, but the purpose of this project is to compare the Message Passing Interface, Open Multi Processing and Open Computing Language on the accelerator Intel Xeon Phi 5120. The comparison was made for the three main problem areas associated with this type of computations. Those are a large number of calculations, the enormity of the data and a multitude of the communication. In order to illustrate this problem, in the context of this project, the total number of twelve applications, four for each of the mentioned areas, had been made. Among implementation there is a serial version, which was the basis used for creation of a MPI, OpenMP and OpenCL versions, so that the comparison was fair and correct. It is worth noting
that in the project offload model was used, so the appropriate algorithm computations were performed only on the coprocessor side.
Applications, which illustrate the problems, are performing vizualization of Mandelbrot set (a large number of simple computations), creation of an image consisting of rectangles which are generated using genetic algorithm (calculations on extensive data), and the simulation of a clientserver architecture (high intensity of communication). At the end of this paper, the results of the comparison were presented, and the best technologies, regarding selected problem areas, were pointed. Optimizations, which could
complement the essence of this project, were also proposed.</p>

<h3>
<a id="main-problem-areas-associate-with-parallel-computations" class="anchor" href="#main-problem-areas-associate-with-parallel-computations" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Main problem areas associate with parallel computations</h3>

<p>The main problem is excelent described in below picture.
<img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/76d16e0c4e99330b4c3ce95db49d6b1b8fec7e4a/images/triangle.png" alt="'Problem triangle', source: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.671.2893&amp;rep=rep1&amp;type=pdf"></p>

<p>As we can see there are three main problems while writing a parallel applications. Problems with communication, with data size and computation complexity. We want to focus on these problems in the next sections.</p>

<h3>
<a id="problem-1st-large-number-of-simple-computations-mandelbrot-set" class="anchor" href="#problem-1st-large-number-of-simple-computations-mandelbrot-set" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem 1st 'Large number of simple computations' Mandelbrot set</h3>

<p>The first algorithm, for which MPI, OpenMP, OpenCL technologies were compared was Mandelbrot algorithm. 
As we mentioned earlier. In the description and clarifications were taken into account the execution times of the entire program, including its components individually to the described technology.</p>

<h2>
<a id="mpi" class="anchor" href="#mpi" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MPI</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Mandelbrot_MPI.png" alt="Mandelbrot set, MPI Implementation"></p>

<h2>
<a id="openmp" class="anchor" href="#openmp" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenMP</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Mandelbrot_OpenMP.png" alt="Mandelbrot set, OpenMP Implementation"></p>

<h2>
<a id="opencl" class="anchor" href="#opencl" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenCL</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Mandelbrot_OpenCL.png" alt="Mandelbrot set, OpenCL Implementation"></p>

<p>Due to the characteristics of the present discussion, that a large intensity calculations in comparison with other algorithms as implemented the best results in terms of performance have been obtained for OpenCL. This is due to the described fact that implementation of this technology rather than run the specified number of threads defined as the global size, allocates tasks to the existing 236 units working. 
As a result, the effect marginalized working thread context switch, as in the case of OpenMP technology. The effect is even greater in the case of MPI, which not only context is switched, but also the processes. It should be noted that the small size of the problems appears to be the best MPI. This is due to model communication between the host and the Xeon Phi coprocessor, wherein the main process communicates with each of the other nodes separately, which causes the natural overlap of computation and data transfer. 
This is compounded by uneven distribution of calculations for static allocation of tasks to the problem Mandelbrot. 
In other technologies, communication occurs only after all calculations on the coprocessor.
Taking into account all the aspects of it can be concluded that for the problem of high intensity calculations, the technology OpenCL is the most optimal.</p>

<h3>
<a id="problem-2nd-calculations-on-extensive-data-genetic-algorithm" class="anchor" href="#problem-2nd-calculations-on-extensive-data-genetic-algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem 2nd 'Calculations on extensive data' Genetic algorithm</h3>

<p>The second compared program was a genetic algorithm. As with other implementation, here also we are taken into account mainly the total execution time of the program, which was created by the following characteristics.</p>

<h2>
<a id="mpi-1" class="anchor" href="#mpi-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MPI</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Genetic_MPI.png" alt="Genetic algorithm, MPI Implementation"></p>

<h2>
<a id="openmp-1" class="anchor" href="#openmp-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenMP</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Genetic_OpenMP.png" alt="Genetic algorithm, OpenMP Implementation"></p>

<h2>
<a id="opencl-1" class="anchor" href="#opencl-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenCL</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Genetic_OpenCL.png" alt="Genetic algorithm, OpenCL Implementation"></p>

<p>In the analysis of execution times environment initialization time was omitted due to the high complexity of the implemented solutions, and what follows, a long time calculations carried out within the framework of the algorithm. These comparisons were taken for the missed cache line to both levels L1 and L2. After the results obtained in the VTune, it was found that the best ratio between the memories misses at the levels L1 and L2 have OpenCL technology where this ratio was about 1 to 5, in OpenMP and MPI was at the level of 1 to 3. In comparison to Mandelbrot algorithm further enhances the technology OpenCL advantage of the greater complexity of the calculations, as well as larger data size. In contrast to the previously described Mandelbrot algorithm in this case OpenCL technology, it was also best for a small number of processes and the small size of the problem. This is due to the previously described fact that it uses the Intel Threading Building Blocks. However, for a large size of the problem and a small number of threads the best technology was OpenMP.</p>

<h3>
<a id="problem-3rd-high-intensity-of-communication-clientserver-app" class="anchor" href="#problem-3rd-high-intensity-of-communication-clientserver-app" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Problem 3rd 'High intensity of communication' ClientServer app</h3>

<p>The last compared program was an application which focused on intensive communication between the host and the coprocessor. At the outset, it is worth noting that the analysis was based, as with other algorithms, at the time of the entire algorithm, along with communication. It should also be noted that the model used in OpenMP implementation is slightly different from that used in other technologies.</p>

<h2>
<a id="mpi-2" class="anchor" href="#mpi-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MPI</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/ClientServer_MPI.png" alt="Client server app, MPI Implementation"></p>

<h2>
<a id="openmp-2" class="anchor" href="#openmp-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenMP</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/ClientServer_OpenMP.png" alt="Client server app, OpenMP Implementation"></p>

<h2>
<a id="opencl-2" class="anchor" href="#opencl-2" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>OpenCL</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/ClientServer_OpenCL.png" alt="Client server app, OpenCL Implementation"></p>

<p>From the analysis of the measurements it came out that the Message Passing Interface technology proved to be the best, because of the attitude of communication between separate processes, which is important in the test applied to the problem of the division of labor. At one time a single process is responsible for one request while other technologies are focused on communication of low bulk granulation. Therefore, assuming a different model, which demands calculation in groups of 16 arrays, especially technology OpenCL could gain because of strong attitudes on the use of technology vectorization. It is worth noting that for a small problem the best technology is OpenMP. However, due to the use of the different model, it omitted mention of her in relation to the MPI.</p>

<h3>
<a id="additional-calculation-with-different-settings-for-mic_kmp_affinity-variable" class="anchor" href="#additional-calculation-with-different-settings-for-mic_kmp_affinity-variable" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Additional calculation with different settings for MIC_KMP_AFFINITY variable</h3>

<h2>
<a id="compact" class="anchor" href="#compact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compact</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Mandelbrot_OpenMP_Compact_MIC_KMP_AFFINITY.png" alt="Mandelbrot set, OpenMP with MIC_KMP_AFFINITY set to COMPACT"></p>

<h2>
<a id="scatter" class="anchor" href="#scatter" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scatter</h2>

<p><img src="https://raw.githubusercontent.com/MNie/ComparisonOfOpenCLOpenMPMPI/17b88f628206a0b1c4984c4fcb53bda83067d425/results/Mandelbrot_OpenMP_Scatter_MIC_KMP_AFFINITY.png" alt="Mandelbrot set, OpenMP with MIC_KMP_AFFINITY set to SCATTER"></p>

<h3>
<a id="authors" class="anchor" href="#authors" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Authors</h3>

<p><a href="https://github.com/MNie" class="user-mention">@MNie</a>
<a href="https://github.com/MiniRoman" class="user-mention">@MiniRoman</a>
<a href="https://github.com/Blasius93" class="user-mention">@Blasius93</a></p>

<h3>
<a id="support-or-contact" class="anchor" href="#support-or-contact" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Support or Contact</h3>

<p>Have any question? Contact us. On <a href="https://github.com/MNie" class="user-mention">@MNie</a> profile you can find email to contact.</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/MNie/ComparisonOfOpenCLOpenMPMPI">Comparison of technologies for parallel computations on Intel Xeon Phi</a> is maintained by <a href="https://github.com/MNie">MNie</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
